import os
import time
import requests
from bs4 import BeautifulSoup
from duckduckgo_search import DDGS
from ingestion.image_search import search_breed  # Import Selenium script

# ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c lÆ°u trá»¯ dá»¯ liá»‡u
DATA_IN_DIR = 'app/utils/data/data_in/'  # ÄÆ°á»ng dáº«n Ä‘áº¿n thÆ° má»¥c lÆ°u bÃ i viáº¿t
MAX_ARTICLES = 5  # Giá»›i háº¡n sá»‘ bÃ i viáº¿t cho má»—i giá»‘ng

# Táº¡o thÆ° má»¥c lÆ°u trá»¯ náº¿u chÆ°a cÃ³
os.makedirs(DATA_IN_DIR, exist_ok=True)

# Danh sÃ¡ch cÃ¡c tá»« khÃ³a vÃ  tháº» cáº§n loáº¡i bá»
EXCLUDE_KEYWORDS = ['advertisement', 'ads', 'sponsored', 'related content']
EXCLUDE_TAGS = ['aside', 'footer', 'nav', 'form']

def format_breed_name(breed):
    return breed.replace(' ', '_')

def clean_content(content):
    soup = BeautifulSoup(content, 'html.parser')

    # Loáº¡i bá» cÃ¡c tháº» khÃ´ng cáº§n thiáº¿t
    for tag in EXCLUDE_TAGS:
        for element in soup.find_all(tag):
            element.decompose()

    paragraphs = soup.find_all('p')
    clean_text = []
    for p in paragraphs:
        text = p.get_text().strip()

        # Loáº¡i bá» cÃ¡c Ä‘oáº¡n chá»©a tá»« khÃ³a quáº£ng cÃ¡o
        if any(keyword in text.lower() for keyword in EXCLUDE_KEYWORDS):
            continue

        clean_text.append(text)

    return '\n'.join(clean_text).strip()

# Kiá»ƒm tra sá»‘ lÆ°á»£ng bÃ i viáº¿t Ä‘Ã£ lÆ°u trong file
def count_existing_articles(breed):
    filepath = os.path.join(DATA_IN_DIR, f"{format_breed_name(breed)}.txt")
    if not os.path.exists(filepath):
        return 0
    
    with open(filepath, 'r', encoding='utf-8') as file:
        content = file.read()
        return content.count('ğŸ“Œ Nguá»“n:')

# LÆ°u bÃ i viáº¿t vÃ o file duy nháº¥t
def save_article(breed, url):
    existing_articles = count_existing_articles(breed)
    if existing_articles >= MAX_ARTICLES:
        return False
    
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        content = clean_content(response.text)

        if content:
            filepath = os.path.join(DATA_IN_DIR, f"{format_breed_name(breed)}.txt")
            article_number = existing_articles + 1
            with open(filepath, 'a', encoding='utf-8') as file:
                file.write(f"\n=== [BÃ i viáº¿t {article_number}] ===\n")
                file.write(f"ğŸ“Œ Nguá»“n: {url}\n\n")
                file.write(content + "\n" + "-" * 50 + "\n")
            print(f"ğŸ“¥ ÄÃ£ lÆ°u bÃ i viáº¿t cho giá»‘ng '{breed}' tá»«: {url}")
            return True
        else:
            print(f"âš ï¸ Ná»™i dung tá»« {url} khÃ´ng há»£p lá»‡ hoáº·c bá»‹ loáº¡i bá».")
            return False

    except Exception as e:
        print(f"âš ï¸ Lá»—i khi táº£i bÃ i viáº¿t tá»« {url}: {e}")
        return False

# Láº¥y cÃ¡c káº¿t quáº£ tÃ¬m kiáº¿m tá»« DuckDuckGo
def get_search_results(query, num_results=20):
    links = []
    try:
        with DDGS() as ddgs:
            results = ddgs.text(query, max_results=num_results)
            links = [result['href'] for result in results]
        time.sleep(1)  # Giáº£m táº§n suáº¥t Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n
    except Exception as e:
        print(f"âš ï¸ Lá»—i khi tÃ¬m kiáº¿m '{query}': {e}")
    return links

# Crawl bÃ i viáº¿t cho giá»‘ng
def crawl_breed(breed):
    existing_articles = count_existing_articles(breed)
    if existing_articles >= MAX_ARTICLES:
        print(f"âœ… ÄÃ£ cÃ³ Ä‘á»§ {MAX_ARTICLES} bÃ i viáº¿t cho giá»‘ng '{breed}', bá» qua...")
        return
    
    print(f"ğŸ” Äang tÃ¬m kiáº¿m bÃ i viáº¿t cho giá»‘ng '{breed}'...")

    search_query = f"{breed} cat breed information"
    links = get_search_results(search_query)

    count = existing_articles
    for link in links:
        if save_article(breed, link):
            count += 1
            if count >= MAX_ARTICLES:
                break

# Láº¥y tÃªn giá»‘ng tá»« Selenium vÃ  cÃ o bÃ i viáº¿t
def crawl_breed_from_image(image_path):
    breed = search_breed(image_path)
    if breed:
        print(f"ğŸ” TÃªn giá»‘ng tÃ¬m tháº¥y: {breed}")
        crawl_breed(breed)
    else:
        print("âŒ KhÃ´ng thá»ƒ tÃ¬m tháº¥y giá»‘ng tá»« hÃ¬nh áº£nh.")

# Cháº¡y crawler cho táº¥t cáº£ giá»‘ng tá»« áº£nh
if __name__ == "__main__":
    # Giáº£ sá»­ báº¡n cÃ³ Ä‘Æ°á»ng dáº«n Ä‘áº¿n cÃ¡c áº£nh
    image_paths = ["path/to/image1.jpg", "path/to/image2.jpg"]  # Äá»•i thÃ nh Ä‘Æ°á»ng dáº«n áº£nh thá»±c táº¿

    for image_path in image_paths:
        crawl_breed_from_image(image_path)
        time.sleep(2)  # Nghá»‰ 2 giÃ¢y giá»¯a cÃ¡c láº§n tÃ¬m kiáº¿m Ä‘á»ƒ trÃ¡nh bá»‹ cháº·n

    print("\nğŸ‰ HoÃ n táº¥t láº¥y dá»¯ liá»‡u!")
